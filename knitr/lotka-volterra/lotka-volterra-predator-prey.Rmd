---
title: "Predator-Prey Population Dynamics: <br /><small>the Lotka-Volterra model in Stan</small>"
author: "Bob Carpenter"
date: "October 16, 2017"
output:
  tufte::tufte_html: default
  html_document:
      includes:
          in_header: stan-rmd.sty
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 2)

library(ggplot2)

library(gridExtra)

library(knitr)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")

library(reshape)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores(logical = FALSE))

library(tufte)

ggtheme_tufte <- function(base_size = 12, base_family = "") {
  theme(plot.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       size = 0.5,
                       linetype = "solid"),
        plot.margin=unit(c(1, 1, 0.5, 0.5), "lines"), 
        panel.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       size = 0.5,
                       linetype = "solid"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.box.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       linetype = "solid"), 
        axis.ticks = element_blank(),
        axis.text = element_text(family = "Palatino", size = 16),
        axis.title.x = element_text(family = "Palatino", size = 20,
                                    margin = margin(t = 15, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(family = "Palatino", size = 20,
                                    margin = margin(t = 0, r = 15, b = 0, l = 0)),
        strip.background = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid"),
        strip.text = element_text(family = "Palatino", size = 16),
        legend.text = element_text(family = "Palatino", size = 16),
        legend.title = element_text(family = "Palatino", size = 16),
        legend.background = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid"),
        legend.key = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid")
  )
}
printf <- function(msg = "%5.3f", ...) {
  cat(sprintf(msg, ...))
}
```



## Abstract

Lotka and Volterra provided parameteric differential equations that characterize the dynamics of the populations of predator and prey species.   A statistical model to account for measurement error and unexplained variation uses the deterministic solutions to the Lotka-Volterra equations as expected population sizes.  Stan is used to encode the model and perform full Bayesian inference.  The model is fit to Canadian lynx^[*Predator*: Canadian lynx
![Canadian lynx](Canadian_lynx_by_Keith_Williams.jpg)
<span style="font-size:70%; float:right">&copy; 2009, Keith Williams, CC-BY 2.0</span>
] and snowshoe hare^[*Prey*: snowshoe hare
![snowshoe hare](Snowshoe_Hare,_Shirleys_Bay.jpg)
<span style="font-size:70%; float:right">&copy; 2013, D. Gordon E. Robertson, CC-BY SA 3.0</span>
] populations between 1900 and 1920, based on the number of pelts collected annually by the Hudson Bay Company.  Posterior predictive checks for replicated data show the model fits this data well.  Full Bayesian inference may be used to estimate future (or past) populations.


# Lynxes and Hares in Canada

> The rise and fall in numbers of snowshoe hares and Canada lynx was observed more than two hundred years ago by trappers working for Hudson’s Bay Company, which was once heavily involved in the fur trade. In the early 20th century, records of the number of lynx and hare pelts traded by Hudson’s Bay were analyzed by biologist Charles Gordon Hewitt.  *(Kara Rogers, 2011, Encyclopedia Brittanica Blog)*

The species of interest are

* [snowshoe hares](https://en.wikipedia.org/wiki/Snowshoe_hare), an hervivorous cousin of rabbits, and
* [Canadian lynxes](https://en.wikipedia.org/wiki/Canada_lynx), a feline predator whose diet consists largely of snowshoe hares.

Hewitt (1921) provides plots of the number of from pelts collected by the Hudson's Bay Company, the largest fur trapper in Canada, between the years of 1821 and 1914.^[Graph from (Hewitt 1921) showing the numbers of pelts captured by the Hudson's Bay Company.  The fluctuations are irregular and the linear growth in rabbits after 1830 is suspicious.
<br /> &nbsp; <br />
![](hewitt-hare-lynx-wolverine.png)
<span style="float:right; font-size: 70%">&copy; Scribner's Sons 1921</span>
]
Hewitt's discussion is also fascinating as to the nature of plagues, lynx migrations, and other factors affecting the populations other than their relative sizes, and factors affecting measurement such as starving lynx being easier to capture.  The models we consider here for illustrative purposes will not consider any of these factors, though they could be extended to do so through the usual strategy of the inclusion of covariates.  Hewitt also discusses many other species;  hares and lynxes occupy only a small part of a single chapter.

Howard (2009) provides numerical data for the number of pelts collected by the Hudson's Bay Company in the years 1900-1920, which we have included in comma-separated value (CSV) form in the case study.

```{r}
lynx_hare_df <-
  read.csv("hudson-bay-lynx-hare.csv", comment.char="#")
```

```{r echo=FALSE, fig.cap = "First and last rows of the long form data."}
lynx_hare_melted_df <- melt(as.matrix(lynx_hare_df[, 2:3]))
colnames(lynx_hare_melted_df) <- c("year", "species", "pelts")
lynx_hare_melted_df$year <-
  lynx_hare_melted_df$year +
  rep(1899, length(lynx_hare_melted_df$year))
knitr::kable(lynx_hare_melted_df[c(1:2, 21:22, 41:42), ],
             full_width = FALSE,
             col.names = c('year', 'species', 'pelts in thousands'),
             align = c('r', 'c', 'r'),
             caption = "Example rows (with their indexes) from the long-form data frame
                        for number of pelts taken by the Hudson Bay Company
                        in the years 1900 to 1920 (in thousands)."  
  ) 
```

```{r echo=FALSE, fig.margin = TRUE, fig.cap="Plot of the lynx and hare populations over time."}
population_plot2 <-
  ggplot(data = lynx_hare_melted_df,
         aes(x = year, y = pelts, color = species)) +
  geom_vline(xintercept = 1900, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  geom_line(size = 0.75) +
  geom_point(size = 1.5) +
  ylab("pelts (thousands)") +
  ggtheme_tufte()
population_plot2
```

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Plot of lynx pelts versus hare pelts over time.  The ups and downs of the individual plots over times is more revealing as a joint plot.  Volterra noticed the spiral pattern was stable and could be modeled with a pair of simple differential equations."}
population_plot1 <-
  ggplot(data = lynx_hare_df,
         aes(x = Lynx, y = Hare, color = Year)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  geom_path(size = 0.75) +
  geom_point(size = 1.5) +
  xlab("lynx pelts (thousands)") +
  ylab("hare pelts (thousands)") +
  ggtheme_tufte()
population_plot1
```

The plot makes it clear that the spikes in the lynx population lag those in the hare population.  When the populations are plotted against one another over time, the population dynamics orbit in an apparently stable pattern.  Volterra (1926) noticed this pattern and realized that it could be modeled with a stable pair of differential equations.

# The Lotka-Volterra Equations

The Lotka-Volterra equations (Volterra 1926, 1927; Lotka 1925) are based on the assumptions that

* the predator population intrinsically shrinks,
* the prey population intrinsically grows,
* larger prey population leads to larger predator population, and
* larger predator population leads to smaller prey populations.

Together, these dynamics lead to a cycle of rising and falling populations.  With a low lynx population, the hare population grows.  As the hare population grows, it allows the lynx population to grow.  Eventually, the lynx population is large enough to start cutting down on the hare population.  That in turn puts downward pressure on the lynx population.  The cycle then resumes from where it started.

The Lotka-Volterra equations (Volterra 1926, 1927; Lotka 1925) are a pair of first-order, ordinary differential equations (ODEs) describing the population dynamics of a pair of species, one predator and one prey.^[Solutions to the Lotka-Volterra equations time plotted as predator vs. prey populations;  the temporal trajectory of populations forms a stable orbit when plotted in two dimensions (Volterra 1926). 
<br /> &nbsp; <br />
![](volterra-1926-cyclic.jpg)
<span style="float:right; font-size:60%">&copy; 1926, Nature Publishing Group</a>
]


* $u(t) \geq 0$ is the population size of the prey species at time $t$, and
* $v(t) \geq 0$ is the population size of the predator species.

Volterra modeled the temporal dynamics of the two species (i.e., population sizes over times) in terms of four parameters, $\alpha, \beta, \gamma, \delta > 0$, as

$$
\begin{eqnarray}
\frac{\mathrm{d}}{\mathrm{d}t} u
& = &  (\alpha - \beta v) u
& = & \alpha u - \beta u v
\\[6pt]
\frac{\mathrm{d}}{\mathrm{d}t} v
& = &  (-\gamma + \delta \, u) \, v
& = & -\gamma v + \delta uv
\end{eqnarray}
$$

As usual in writing differential equations, $u(t)$ and $v(t)$ are rendered as $u$ and $v$ to simplify notation.


## Positivitity constraint and extinction

As population measures, the values of $u(t)$ and $v(t)$ should be non-negative.  Nothing in the differential equations here explicitly enforces positivity.  Nevertheless, as long as the initial populations are non-negative, i.e., $u(0) \geq 0$ and $v(0) \geq 0$, the values $u(t)$ and $v(t)$ for other times $t$ must also be positive.  This result is easy to see from the form of the differential equations, which take the change in each population to be a factor of the population size itself.  

## Behavior in the limit

One way to understand systems of equations is to consider their limiting behavior.  Here, there are four possible limiting behaviors

1. If both populations begin at size zero, they remain that way.
3. If only the predator population begins at zero, the prey population grows without bound.
4. If neither population is initially zero, the population sizes will oscillate in a fixed pattern indefinitely.

In the oscillating cases, the population sizes may come arbitrarily close to zero, but they will never quite die out because of the form of the equations.

The fact that we can have very small fractional population size estimates is unrealistic in multiple ways.  Most obviously, animal populations are natural numbers, not fractional;  this is usually a tolerable abstraction.^[To solve this problem, the populations could be drawn as Poisson or negative binomial using the continuous expected counts.  With counts in the thousands, a normal approximation is more than adequate and more convenient computationally.]   More importantly from a biological point of view, random events such as accidents can drive a very small population extinct.


# Measurement Error and Unexplained Variation

The Lotka-Volterra model is deterministic.  Given the system parameters and the initial conditions, the solutions to the equations are fully determined.  Population measurements are not so well behaved that they exactly follow the model.  As we saw from the plots, the measurements do roughly follow the model for various ranges of parameters, but there is residual error that cannot be explained by the model.

There are factors that impact predator and prey population size other than the current population size.  There are variable environmental effects, such as annual weather, which will vary from season to season and year to year and affect population sizes.  Infectious diseases occasionally spread through a population, reducing its size (Hewitt 1921).  There are also more long-term environmental factors such as carrying capacity.^[Carrying capacity is roughly the maximum population that an environment can sustain.  It is often modeled in the system dynamics as an asymptote on population size.]  In addition to factors that affect population which are not modeled, there is typically noise in the measurements.  

In this particular case, we cannot even measure the population directly.  Instead, pelts are used as a proxy for population sizes.^[It would be possible to combine population models such as these with mark-recapture population models.]  Hewitt (1921) further noted that animals are easier to trap when food is scarce, making the population measurements dependent on the population size.  Even the exact number of pelts taken is likely to be only approximate, as they were collected in a range of locations over an entire season.  Here, the number is only recorded to the closest 100.^[Rounding can be treated explicitly by allowing the true value to be a parameter;  for instance if rounding is to the closest 100, the parameter can be modeled as uniform over $\pm 50$ of the true measured value or even greater if the measurement on which the rounding was based may be in error.]

# Solving the inverse problem

For a given legal value of the model parameters and initial state,  the Lotka-Volterra model predicts population dynamics into the future (and into the past).  But given noisy data about population dynamics, how do we solve the inverse problem, that of inferring the values of model parameters consistent with the data?  The general approach in Bayesian statistics is somewhat counterintuitive, as it involves formulating the forward model then using general principles to solve the inverse problem.   Specifically, it requires specifying what we know about the parameters before seeing the data, and what we expect the data to look like given parameter values.  Mathematically, this is done through a prior $p(\theta)$ over the sequence of parameters $\theta$ that encapsulates our knowledge of the parameters before seeing the data along with a sampling distribution $p(y|\theta)$ that characterizes the distribution of observable data $y$ given parameters $\theta$.^[The sampling distribution is called a likelihood function when considered as a function of parameters $\theta$ for fixed data $y$.] 

Bayes's rule gives us a general solution to the inverse problem in the form of a posterior probability function $p(\theta | y)$.^[The derivation of Bayes's rule for parameters $\theta$ and observed data $y$ is $$ \begin{array}{rcl} p(\theta | y) & = & \displaystyle \frac{p(\theta, y)}{p(y)} \\[8pt] & = & \displaystyle \frac{p(y | \theta) \, p(\theta)}{p(y)} \\[8pt] & = & \displaystyle \frac{p(y | \theta) \, p(\theta)}{\int p(y | \theta) \, p(\theta) \, \mathrm{d}\theta} \\[8pt] & \propto & p(y | \theta) \, p(\theta). \end{array} $$]



# Noise model for Lotka-Volterra dynamics

## A linear regression analogy

Like a simple linear regression, or non-linear generalized linear model, the trick is to treat the underlying determinstic model as providing a value which is expected to have error from both measurement and unexplained variance due to the simplifications in the scientific model.  Consider the typical formulation of a linear regression, where $y_n$ is an observable scalar outcome, $x_n$ is a row vector of unmodeled predictors, $\beta$ is a coefficient vector parameter, and $\sigma > 0$ is the error scale, 

$$
\begin{eqnarray}
y_n & = & x_n \beta + \epsilon_n
\\[6pt]
\epsilon_n & \sim & \mbox{Normal}(0, \sigma)
\end{eqnarray}
$$

The deterministic part of the equation is the linear predictor $x \beta$.  The stochastic error term, $\epsilon_n$, gets a normal distribution located at zero with scale parameter $\sigma > 0$ (this error model ensures that the maximum likelihood value for $\beta$ is at the least squares solution).  We typically formulate this model without the latent error variable $\epsilon_n$ as follows,^[The latent error variable may be defined as  $\epsilon_n = y_n - x_n \beta$.]

$$
y_n \sim \mbox{Normal}(x_n \beta, \sigma).
$$


## Lotka-Volterra error model

The data $y_i$ consists of measurements of the prey $y_{i, 1}$ and predator $y_{i, 2}$ populations at times $t_i$.  The Lotka-Volterra equations will replace the determinsitic parts of the linear regression equations.

The true population sizes at time $t = 0$ are unknown---we only have measurements $y^{\rm init}_1$ and $y^{\rm init}_2$ for it.  The true initial population sizes at time $t = 0$ will be represented by a parameter $z^{\mathrm init}$, so that

$$
z^{\mathrm init}_1 = u(t = 0)
\ \ \ \mathrm{and} \ \ \
z^{\mathrm init}_2 = v(t = 0).
$$

Next, let $z_1, \ldots, z_N$ be the solutions to the Lotka-Volterra differential equations at times $t_1, \ldots, t_N$ given initial conditions $z(t = 0) = z^{\mathrm init}$ and parameters $\theta = (\alpha, \beta, \gamma, \delta)$.  Each $z_n$ is a pair of prey and predator population sizes at the specified times,

$$
z_{n, 1} = u(t_n)
\ \ \ \mathrm{and} \ \ \
z_{n, 2} = v(t_n).
$$

The $z_n$ are random variables, but they are deterministic functions of the random variables for the initial state $z^{\mathrm init}$ and system parameters $\alpha, \beta, \gamma, \delta$.

The observed data is the form of measurements $y^{\rm init}$ of the initial population of prey and predators, and subsequent measurements $y_n$ at times $t_n$, where $y^{\mathrm init}$ and the $y_n$ consist of a pair of measured population sizes, for the prey and predator species.

Putting this all together, the measurements $y^{\rm init}$ and $y_n$ are of a underlying latent population sizes of predator and prey $z^{\mathrm init} = u(0), v(0)$ and $z_n = u(t_n), v(t_n)$.  

## Multiplicate error and the lognormal distribution

It is natural for positive-only parameters to log transform them so that they are no longer constrained to be positive.  The simplest assumption we can make here is to take the error to be additive on the log scale, just as in a linear regression.

$$
\begin{eqnarray}
\log y_{n, k} & = & \log z_{n, k} + \epsilon_{n, k}
\\[6pt]
\epsilon_{n, k} & \sim & \mathrm{Normal}(0, \sigma_k)
\end{eqnarray}
$$

where the $z_n$ are the solutions to the Lotka-Volterra equations at times $t_1, \ldots, t_N$ given initial population $z^{\mathrm init}$.  The prey and predator populations have error scales (on the log scale) of $\sigma_1$ and $\sigma_2$.

```{r echo=FALSE, fig.margin=TRUE, fig.cap="The top figure shows the density of a standard normal variate, i.e., location (mean) 0 and scale (standard deviation) 1. The bottom figure the corresponding standard lognormal variate, i.e., log location 0 and log scale 1.  The central 99.5% interval is shown in both cases with a dashed vertical line indicating the median and dotted lines the boundaries of the central 95% interval.  The skew in the lognormal plot is due to the exponentiation---the range (2, 3) in the standard normal plot, containing only 2% of the total probability mass, maps to (7, 20) in the lognormal plot.  If the horizontal and vertical axes were drawn to the same scale, the areas under the two curves would be the same."}
lb = -3
ub = 3
plot1 <- ggplot(data.frame(x = c(lb, ub)), aes(x = x)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1)) +
  scale_y_continuous(name = expression(p(log~~y[n]))) +
  scale_x_continuous(name = expression(log~~y[n])) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = -2, linetype = "dotted") +
  geom_vline(xintercept = 2, linetype = "dotted") +
  ggtheme_tufte()  +
  theme(axis.text.y = element_blank())
#
plot2 <- ggplot(data.frame(x = c(exp(lb), exp(ub))), aes(x = x)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  stat_function(fun = function(x) dlnorm(x, meanlog = 0, sdlog = 1)) +
  scale_y_continuous(name = expression(p(y[n]))) +
  scale_x_continuous(breaks = c(0, 1, 5, 10, 20), name = expression(y[n])) +
  geom_vline(xintercept = 1, linetype = "dashed") +
  geom_vline(xintercept = exp(-2), linetype = "dotted") +
  geom_vline(xintercept = exp(2), linetype = "dotted") +
  ggtheme_tufte() +
  theme(axis.text.y = element_blank())
grid.arrange(plot1, plot2, nrow=2, ncol=1)
```



With log-transformed measurements with additive errors, transforming back to the natural scale leads to multiplicative errors,

$$
\begin{array}{rcl}
y_{n, k} & = & \exp(\log z_{n, k} + \epsilon_{n, k})
\\[4pt]
         & = & z_{n, k} \, \exp(\epsilon_{n,k})
\end{array}
$$

with $\exp(\epsilon_{n,k})$ being constrained to be positive, so that the product is positive.  In other words, rather than the measurement being plus or minus some value, it's plus or minus some percentage of the total.

This transform and error multiplicative error model are so common that they are jointly known as the lognormal distribution, so that we can simplify the above notation as we did with linear regression and write

$$
y_{n, k} \sim \mathsf{LogNormal}(z_{n, k}, \sigma_n).
$$
if 
$$
\log y_{n, k} \sim \mathsf{Normal}(z_{n, k}, \sigma_n).
$$


# Weakly informative priors

The only remaining part of the model to formulate is what we know about the parameters.  In general, we^[The Stan Development Team, in a Wiki on  [prior choice recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations).] recommend at least weakly informative priors on parameters.   In practice, weakly informative priors inform the scale of the answer, but not the exact value.  That means that in order to formulate such priors, we need to know the rough scale on which to expect an answer.  Because this model is well understood and widely used, as is the basic behavior of predator and prey species such as lynxes and hares, the parameter ranges for the Lotka-Volterra model leading to stable and realistic population oscillations are well known.

## Priors for system parameters

For the parameters, recall that $\alpha$ and $\gamma$ are multipliers of $u$ and $-v$ respectively in the state equations, whereas $\beta$ and $\delta$ are multipliers of the product $u \, v$.  Combined with the fact that the scale of $u$ and $v$ are roughly 10 as the data has been encoded, leads to a prior scaling of the former parameters by the same factor.  

$$
\begin{eqnarray}
\alpha, \gamma & \sim & \mathrm{Normal}(1, 0.5)
\\[6pt]
\beta, \delta & \sim & \mathrm{Normal}(0.05, 0.05)
\end{eqnarray}
$$

## Prior for noise scale

The noise scale is proportional, so the following prior should be weakly informative, as a value smaller than 0.1 or larger than 1 would be unexpected. Because values are positive, this prior adopts the lognormal distribution.^[A lognormal prior on $\sigma$ is not consistent with zero values of $\sigma$, but we do not expect zero values of $\sigma$ here as we do not expect this model to be particularly accurate as formulated.]  Another reasonable prior choice would be a broad half normal.^[An interval prior with hard boundaries is not recommended as it may bias results if the data is compatible with results near the boundary.]

$$
\sigma \sim \mathrm{LogNormal}(-1, 1)
$$

```{r echo=FALSE, fig.margin=TRUE, fig.cap = "Plot of lognormal(-1, 1) prior.  The median, 0.4, is highlighted with a dashed line and the central 95% interval, (0.05, 2.7), with dotted lines.  This prior density is not consistent with values of sigma approaching zero."}
loc = -1
scale = 1
ggplot(data.frame(x = c(0, 4)), aes(x = x)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  stat_function(fun = function(x) dlnorm(x, meanlog = loc, sdlog = scale)) +
  geom_vline(xintercept = exp(loc), linetype = "dashed", color="darkgrey") +
  geom_vline(xintercept = exp(loc - 2 * scale), linetype = "dotted", color="darkgrey") +
  geom_vline(xintercept = exp(loc + 2 * scale), linetype = "dotted", color="darkgrey") +
  scale_y_continuous(name = expression(p(sigma))) +
  scale_x_continuous(name = expression(sigma)) +
  ggtheme_tufte() +
  theme(axis.text.y = element_blank())
```

For the initial population of predator and prey, the following priors are weakly informative.

$$
z^{\mathrm init}_1, \ z^{\mathrm init}_2 \ \sim \ \mathsf{LogNormal}(\log(10), 1)
$$

```{r echo=FALSE, fig.margin=TRUE, fig.cap = "Plot of lognormal(log(10), 1) prior.  The median, 2.3, is highlighted with a dashed line and the 95% interval, (1.4, 75), withdotted lines. This prior suffices for both predator and prey sizes."}
loc = log(10)
scale = 1
ggplot(data.frame(x = c(0, 150)), aes(x = x)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  stat_function(fun = function(x) dlnorm(x, meanlog = log(10), sdlog = 1)) +
  geom_vline(xintercept = exp(loc), linetype = "dashed", color="darkgrey") +
  geom_vline(xintercept = exp(loc - 2 * scale), linetype = "dotted", color="darkgrey") +
  geom_vline(xintercept = exp(loc + 2 * scale), linetype = "dotted", color="darkgrey") +
  scale_y_continuous(name = expression(p(z^init), )) +
  scale_x_continuous(name = expression(z^init)) +
  ggtheme_tufte() +
  theme(axis.text.y = element_blank())
```


# Stan Code


## Coding the system dynamics

Whenever a system of differential equations is involved, the system equations must be coded as a Stan function.  In this case, the model is relatively simple as the state is only two dimensional and there are only four parameters.  Stan requires the system to be defined with exactly the signature defined here for the function <code>dz_dt()</code>.  The first argument is for time, which is not used here because the Lotka-Voltarra equations are not time-dependent.  The second argument is for the system state, and here it is coded as an array
$z = (u, v)$.  The third argument is for the parameters of the equation, of which the Lotka-Voltarra equations have four, which are coded as $\theta = (\alpha, \beta, \gamma, \delta)$.  The fourth and fifth argument are for data constants, but none areneeded here, so these arguments are unused.

```
  real[] dz_dt(real t,       // time (unused)
               real[] z,     // system state
               real[] theta, // parameters
               real[] x_r,   // data (unused)
               int[] x_i) {
    real u = z[1];
    real v = z[2];

    real alpha = theta[1];
    real beta = theta[2];
    real gamma = theta[3];
    real delta = theta[4];

    real du_dt = (alpha - beta * v) * u;
    real dv_dt = (-gamma + delta * u) * v;

    return { du_dt, dv_dt };
  }
```

After unpacking the variables from their containers, the derivatives of population with respect to time are defined just as in the mathematical specification.  The return value uses braces to construct the two-element array to return, which consists of the derivatives of the system components with respect to time,

$$
\frac{\mathrm{d}}{\mathrm{d}t} z
\ = \ \frac{\mathrm{d}}{\mathrm{d}t} (u, v)
\ = \ \left( \frac{\mathrm{d}}{\mathrm{d}t} u, \, \frac{\mathrm{d}}{\mathrm{d}t} v \right).
$$

The data and parameters are coded following their specifications.

```
data {
  int<lower = 0> N;           // num measurements
  real ts[N];                 // measurement times > 0
  real y_init[2];             // initial measured population
  real<lower = 0> y[N, 2];    // measured population at measurement times
}
parameters {
  real<lower = 0> theta[4];   // theta = { alpha, beta, gamma, delta }
  real<lower = 0> z_init[2];  // initial population
  real<lower = 0> sigma[2];   // error scale
}
```

The solutions to the Lotka-Volterra equations for a given initial state $z^{\mathrm init}$ are coded up as transformed parameters.  This will allow them to be used in the model and inspected in the output.  It also makes it clear that they are all functions of the initial population and parameters (as well as the solution times).^[Stan provides two solvers, a Runge-Kutta 4th/5th-order solver (`_rk45`) and a backwards-differentiation formula solver (`_bdf`) for stiff systems.  The RK45 solver is twice as fast as the BDF solver for this data.   The symptom of stiffness is slow iterations that may appear to be hanging if a large number of iterations is permitted.  If the solver runs into stiffness and more careful initialization or prior choice does not mitigate the problem, the BDF solver may be used to adjust for the ill-conditioned Jacobians at the cost of increased computation time.]

```
transformed parameters {
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
}
```

The required real and integer data arguments in the second line are both given as constant size-zero arrays.  The last line provides relative and absolute tolerances, along with the maximum number of steps allowed in the solver before rejecting.^[Rejections behave like zero density points from which no progress may be made;  in C++, the behavior is to throw an exception, which is caught and displayed as an error message by the interfaces.]  For further efficiency, the tolerances for the differential equation solver are relatively loose for this example; usually tighter tolerances are required (smaller numbers).

With the solutions in hand, the only thing left are the prior and likelihood.  As with the other parts of the model, these directly follow the notation in the mathematical specification of the model.

```
model {
  sigma ~ normal(0, 0.5);
  theta[1:2] ~ normal(0, 1);
  theta[3:4] ~ normal(0, 0.2);
  z_init[1] ~ normal(10, 10);
  z_init[2] ~ normal(50, 50);

  y_init ~ lognormal(log(z_init), sigma);
  for (k in 1:2)
    y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
}
```

## Fitting the Hudson's Bay Company data

First, the data is setup in a form suitable for Stan.

```{r}
N <- length(lynx_hare_df$Year) - 1
ts <- 1:N
y_init <- c(lynx_hare_df$Hare[1], lynx_hare_df$Lynx[1])
y <- as.matrix(lynx_hare_df[2:(N + 1), 2:3])
y <- cbind(y[ , 2], y[ , 1]); # hare, lynx order
lynx_hare_data <- list(N, ts, y_init, y)
```

Next, the model is translated to C++ and compiled.

```{r results="hide"}
model <- stan_model("lotka-volterra.stan")
```

Finally, the compiled model and data are used for sampling.^[The independent steps of compiling the Stan model and running it are highlighted here.  These two steps may be composed into a single call as `stan("lotka_volterra.stan", data = lynx_hare_dat, seed = 123)`.  Calling `rstan_options(auto_write = TRUE)` configures `stan_model()` and `stan()` to reuse a compiled model if it hasn't changed.]  Stan's default settings are sufficient for this data set and model.

```{r results="hide"}
fit <- sampling(model, data = lynx_hare_data, seed = 123)
```

The output can be displayed in tabular form, here limited to the median (0.5 quantile) and 80% interval (0.1 and 0.9 quantiles), and restricted to the parameters of interest.

```{r}
print(fit, pars=c("theta", "sigma", "z_init"), 
      probs=c(0.1, 0.5, 0.9), digits = 3)
```

There were no divergent transitions^[Divergences occur when Stan's Hamiltonian solver diverges from the Hamiltonian because of numerical problems in the gradient-based approximation of the log density.] reported and R-hat values are all near 1, which is consistent with convergence.  The effective sample size estimates for each parameter are sufficient for inference.^[With effective sample sizes of roughly one thousand, standard errors are roughly one thirtieth the size of posterior standard deviations.]  Thus we have reason to trust that Stan has produced an adequate approximation of the posterior.


## Comparing the fitted model to data

Using a non-statistically motivated error term and optimization, Howard (2009, Figure 2.10) provides the following approximate point estimates for the model parameters based on the data.
$$
\alpha^* = 0.55, \ \
\beta^* = 0.028, \ \
\gamma^* = 0.84, \ \
\delta^* = 0.026
$$

Our model produced the following point estimates based on the posterior mean,^[The posterior mean minimizes expected squared error, whereas posterior medians minimize expected absolute error.  Here, the mean and median are the same to within MCMC standard error.]

$$
\hat{\alpha} = 0.57, \ \
\hat{\beta} = 0.029, \ \
\hat{\gamma} = 0.76, \ \
\hat{\delta} = 0.022,
$$

which are quite close to Howard's estimates.^[Discrepancies are to be expected in that we are finding a posterior mean wheras Howard is finding a posterior mode.  We do not suspect the priors have a strong influence here, but this could be checked by varying them and comparing results, i.e., performing a sensitivity analysis.] The posterior intervals are quite large in fact. The posterior intervals may be interpreted probabilistically,

$$
\begin{array}{ccc}
\mbox{Pr}[0.48 \leq \alpha \leq 0.66] & = & 0.8
\\[4pt]
\mbox{Pr}[0.23 \leq \beta \leq 0.35] & = & 0.8
\\[4pt]
\mbox{Pr}[0.65 \leq \delta \leq 0.88] & = & 0.8
\\[4pt]
\mbox{Pr}[0.018 \leq \gamma \leq 0.027] & = & 0.8
\end{array}
$$

Error scales for both populations have the same posterior mean estimate,

$$
\hat{\sigma}_1 \ = \ \hat{\sigma}_2 \ = \ 0.25.
$$

and both have the same posterior 80% interval, (0.20, 0.31).^[This suggests they may be completely pooled and modeled using a single parameter.]


## Inference for population sizes

One inference we would like to make from our data is the size of the lynx and hare populations over time.  Howard (2009)
plugs in optimization-derived point estimates to derive population predictions.  

Rather than plugging in point estimates to get point predictions, we will follow the fully Bayesian approach of adjusting for uncertainty.  This uncertainty takes two forms in inference.  First, there is estimation uncertainty, which is fully characterized by the joint posterior density $p(\alpha, \beta, \gamma, \delta, z^{\mathrm init}, \sigma \mid y)$.  In well-behaved models in which the data is broadly consistent with the prior and likelihood, estimation uncertainty is reduced by larger samples; as more data is available, the posterior will concentrate  

The second form of uncertainty is the observation error and unexplained variation, which are both rolled into a single sampling distribution, $\log y_n \sim \mathsf{Normal}(\log z_n, \sigma)$.  As in the Stan implementation, $z_n = (u_n, v_n)$ is the solution to the differential equation conditioned on the parameters $\theta = (\alpha, \beta, \gamma, \delta, \sigma)$ and initial state $z^{\mathrm init}$.  

## Posterior predictive checks

We use posterior predictive checks to evaluate how well our model fits the data on which it is trained.  The basic idea is to take the posterior for the fitted model and use it to predict what the data should've looked like.  That is, we will be replicating new $y$ values that parallel the actual observations $y$.  Becuase they are replicated values, we write them as as $y^{\mathrm{rep}}$.  The distribution of these replicated values is given by the posterior predictive distribution, 

$$
p(y^{\mathrm{rep}} | y)
\ = \
\int p(y^{\mathrm{rep}} | \theta) \ p(\theta | y) \ \mathrm{d}\theta.
$$

where $\theta = (\alpha, \beta, \gamma, \delta, z^{\mathrm init}, \sigma)$ is the vector of parameters for the model.  Our two forms of uncertainty are represented in the two terms in the integral.  The first is the sampling distribution for the replications, $p(y^{\mathrm rep} | \theta)$, which is the distribution of observations $y^{\mathrm rep}$ given parameters $\theta$.  This term encapsulates the unexplained variance and measurement error. The second term is the posterior $p(\theta | y)$, which encapsulates our uncertainty in our parameter estimates $\theta$ given the observations $y$.  What the integral is telling us is that we're calculating a weighted average of the sampling distribution, with weights given the posterior.  In statistical terms, we are calculating an expectation of a function of the parameters, $f(\theta) = p(y^{\mathrm rep} | \theta)$, over  the posterior $p(\theta | y)$, which can be written concisely as a conditional expectation,

```{r echo=FALSE, fig.margin = TRUE, fig.cap = "Posterior predictive checks, including posterior means and 50% intervals along with the measured data.  If the model is well calibrated, as this one appears to be, 50% of the points are expected to fall in their 50% intervals."}
z_init_draws <- extract(fit)$z_init
z_draws <- extract(fit)$z
y_init_rep_draws <- extract(fit)$y_init_rep
y_rep_draws <- extract(fit)$y_rep
predicted_pelts <- matrix(NA, 21, 2)
min_pelts <- matrix(NA, 21, 2)
max_pelts <- matrix(NA, 21, 2)
for (k in 1:2) {
  predicted_pelts[1, k] <- mean(y_init_rep_draws[ , k])
  min_pelts[1, k] <- quantile(y_init_rep_draws[ , k], 0.25)
  max_pelts[1, k] <- quantile(y_init_rep_draws[ , k], 0.75)
  for (n in 2:21) {
    predicted_pelts[n, k] <- mean(y_rep_draws[ , n - 1, k])
    min_pelts[n, k] <- quantile(y_rep_draws[ , n - 1, k], 0.25)
    max_pelts[n, k] <- quantile(y_rep_draws[ , n - 1, k], 0.75)
  }
}

lynx_hare_melted_df <- melt(as.matrix(lynx_hare_df[, 2:3]))
colnames(lynx_hare_melted_df) <- c("year", "species", "pelts")
lynx_hare_melted_df$year <-
  lynx_hare_melted_df$year +
  rep(1899, length(lynx_hare_melted_df$year))

Nmelt <- dim(lynx_hare_melted_df)[1]
lynx_hare_observe_df <- lynx_hare_melted_df
lynx_hare_observe_df$source <- rep("measurement", Nmelt)

lynx_hare_predict_df <-
  data.frame(year = rep(1900:1920, 2),
             species = c(rep("Lynx", 21), rep("Hare", 21)),
             pelts = c(predicted_pelts[, 2],
                       predicted_pelts[, 1]),
             min_pelts = c(min_pelts[, 2], min_pelts[, 1]),
             max_pelts = c(max_pelts[, 2], max_pelts[, 1]),
             source = rep("prediction", 42))

lynx_hare_observe_df$min_pelts = lynx_hare_predict_df$min_pelts
lynx_hare_observe_df$max_pelts = lynx_hare_predict_df$max_pelts

lynx_hare_observe_predict_df <-
  rbind(lynx_hare_observe_df, lynx_hare_predict_df)

population_plot2 <-
  ggplot(data = lynx_hare_observe_predict_df,
         aes(x = year, y = pelts, color = source)) +
  geom_vline(xintercept = 1900, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  facet_wrap( ~ species, ncol = 1) +
  geom_ribbon(aes(ymin = min_pelts, ymax = max_pelts),
	      colour = NA, fill = "black", alpha = 0.1) +
  geom_line() +
  geom_point() +
  ylab("pelts (thousands)") +
  ggtheme_tufte() +
  theme(legend.position="bottom")

population_plot2
```


$$
p(y^{\mathrm{rep}} | y)
\ = \
\mathbb{E}\!\left[ p(y^{\mathrm{rep}} | \theta) \ \big| \ y \right].
$$

As with other posterior expectations, the Bayesian point estimate is given by a simple average over simulated values, where $y^{\mathrm{rep}(m)}$ is just the result of simulating the value of $y^{\mathrm{rep}}$ according to the generative model based on parameter draw $\theta^{(m)}$.


# How large are the populations?

Going on the assumption that the number of pelts collected is proportional to the population, we only know how the relative sizes of the populations change, not their actual sizes.

This model could be combined with a mark-recapture model to get a better handle on the actual population size.  Mark-recapture gives you an estimate of actual numbers and the Lotka-Volterra model would provide information on relative change in the predator and prey populations.

## Predicted population cycles

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Plot of expected population orbit for one hundred draws from the posterior.  Each draw represents a different orbit.  If the solutions were requesed at more fine grained intervals than yearly, the results would appear smooth."}
ss <- extract(fit)
df <- data.frame(list(lynx = ss$z[1, 1:12 , 1], hare = ss$z[1, 1:12, 2], draw = 1))
for (m in 2:100) {
  df <- rbind(df, data.frame(list(lynx = ss$z[m, 1:12 , 1], hare = ss$z[m, 1:12, 2], draw = m)))
}  
plot <- ggplot(df) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  geom_path(aes(x = lynx, y = hare, colour = draw), size = 0.75, alpha = 0.2) +
#  geom_point(size = 1.25) +
  xlab("lynx pelts (thousands)") +
  ylab("hare pelts (thousands)") +
  ggtheme_tufte() +
  theme(legend.position="none")
plot
```
In the same way as Volterra (1926) plotted the cycles of predator and prey populations, we can select draws from the posterior and plot them. The variation here is due to posterior uncertainty in the value of the system parameters $\alpha, \beta, \gamma, \delta$ and the initial population $z^{\mathrm init}$.

## Predicted measurements

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Plot of expected pelt collections for one hundred draws from the posterior.  Each draw represents a different orbit (as shown in the previous figure) multiplied (not added) by an error term.   Even if the solutions were requesed at more fine grained intervals than yearly, the results would not appear smooth due to the error term.  Some of the extreme draws are typical when large values have high error on the multiplicative scale."}
ss <- extract(fit)
df <- data.frame(list(lynx = ss$y_rep[1, 1:12 , 1], hare = ss$y_rep[1, 1:12, 2], draw = 1))
for (m in 2:100) {
  df <- rbind(df, data.frame(list(lynx = ss$y_rep[m, 1:12 , 1], hare = ss$y_rep[m, 1:12, 2], draw = m)))
}  
plot <- ggplot(df) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  geom_path(aes(x = lynx, y = hare, colour = draw), size = 0.75, alpha = 0.2) +
  xlab("lynx pelts (thousands)") +
  ylab("hare pelts (thousands)") +
  ggtheme_tufte() +
  theme(legend.position="none")
plot
```

In addition to the estimation uncertainty discussed in the previous section, there is also the general error due to measurement error, model misspecification, etc.  In order to simulate the number of pelts that are reported collected (which may itself have error relative to the actual number of pelts), we must additionally consider the general error term.  That is already rolled into the variables $y^{\mathrm rep}$, so we plot those here.



# Extensions

The Lotka-Volterra model is easily extended for realistic applications in several ways.

1.  Predictors can be rolled into the system state to take into the dynamnics to account for things like the correlation of populations with the abundance of food.
1.  The model may be extended beyond two species.  The dynamics for each species will reflect that it may stand in predator-prey relations to multiple other species.
1.  Additional data for population observations may be included, such as adding a mark-recapture model for tag-release-recapture data of populations.


# Exercises

I ran out of steam before turning this case study into a self-contained tutorial on Bayesian modeling.  So I leave the next steps to you.

1.  *Forecasting and backcasting*.  Extend predictions another 50 years into the future and plot as in the last plot.  This can be done by extending the solution points in the transformed parameters, but is more efficiently done in the generated quantities block.  Next, extend the predictions 50 years into the past.^[Hint: the initial time will have to be changed in the call to `ode_integrate`.]
1.  *Model calibration*.  Write a Stan model to simulate data from this model.  First simulate parameters from the prior (or pick ones consistent with the priors).  Then simulate data from the parameters.  Finally, fit the model in Stan and compare the coverage as in the last plot in the case study.^[In general, this technique was extended and converted to a statistical test by Cook, Gelman and Rubin (2006).  As extra credit, apply this test to validate the model.].
1.  *Missing data*.  Suppose that several of the measurements are missing.  Write a Stan program that uses only the observed measurements.  How robust is the fit to missing a few data points?
1.  *Error model.* Replace the lognormal error with a simple normal error model.  What does this do to the `z` estimates and to the basic parameter estimates?  Which error model fits better?
1.  *Prior choice*.  Perform a sensitivity analysis on the prior choices made for this model.  When the prior means or scales are varied, how much does the posterior vary?  Does the model become easier or harder to fit (in terms of effective sample size per unit time or divergent transitions) with different prior choices?  What does this imply about the number of digits with which we report results and thus the effective sample sizes necessary for most inferences?


# References

* Cook, S. R., Gelman, A., & Rubin, D. B. (2006)  Validation of software for Bayesian models using posterior quantiles. *Journal of Computational and Graphical Statistics*, 15(3), 675--692.
* Hewitt, C. G. (1921) [*The Conservation of the Wild Life of Canada*](https://books.google.com/books?id=8hVDAAAAIAAJ). Charles Scribner's Sons.
* Howard, P. (2009). [Modeling Basics](http://www.math.tamu.edu/~phoward/m442/modbasics.pdf). Lecture Notes for Math 442, Texas A&M University.
* Lotka, A. J. (1925). *Principles of physical biology*. Baltimore: Waverly.
* Rogers, K. (2011) [The rise and fall of the Canada lynx and snowshoe hare]( http://blogs.britannica.com/2011/06/rise-fall-canada-lynx-snowshoe-hare/). *Encyclopedia Brittanica Blog*.
* Volterra, V. (1926). Fluctuations in the abundance of a species considered mathematically. *Nature*, 118(2972), 558-560.
* Volterra, V. (1927). *Variazioni e fluttuazioni del numero d'individui in specie animali conviventi*. C. Ferrari.



<br />

## Appendix

Here is the complete Stan program for solving the inverse problem for the Lotka-Volterra model, including posterior predictive checks. 

```
functions {
  real[] dz_dt(real t,       // time
               real[] z,     // system state {prey, predator}
               real[] theta, // parameters
               real[] x_r,   // unused data
               int[] x_i) {
    real u = z[1];
    real v = z[2];

    real alpha = theta[1];
    real beta = theta[2];
    real gamma = theta[3];
    real delta = theta[4];

    real du_dt = (alpha - beta * v) * u;
    real dv_dt = (-gamma + delta * u) * v;

    return { du_dt, dv_dt };
  }
}
data {
  int<lower = 0> N;         // number of measurement times
  real ts[N];               // measurement times > 0
  real y_init[2];           // initial measured populations
  real<lower = 0> y[N, 2];  // measured populations
}
parameters {
  real<lower = 0> theta[4];  // theta = { alpha, beta, gamma, delta }
  real<lower = 0> z_init[2]; // initial population
  real<lower = 0> sigma[2];  // measurement errors
}
transformed parameters {
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-5, 1e-3, 5e2);
}
model {
  theta[{1, 3}] ~ normal(1, 0.5);
  theta[{2, 4}] ~ normal(0.05, 0.05);

  z_init ~ lognormal(10, 1);
  sigma ~ lognormal(-1, 1);

  for (k in 1:2) {
    y_init[k] ~ lognormal(log(z_init[k]), sigma[k]);
    y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
  }
}
generated quantities {
  real y_init_rep[2];
  real y_rep[N, 2];

  for (k in 1:2) {
    y_init_rep[k] = lognormal_rng(log(z_init[k]), sigma[k]);
    for (n in 1:N)
      y_rep[n, k] = lognormal_rng(log(z[n, k]), sigma[k]);
  }
}
```


## Appendix: Session information

```{r}
sessionInfo()
```


## Appendix: Licenses

* <span style="font-size:85%">Code &copy; 2017--2018, Columbia University, licensed under BSD-3.</span>
* <span style="font-size:85%">Text &copy; 2017--2018, Bob Carpenter, licensed under CC-BY-NC 4.0.</span>
